I"”-<p>å‰é¢å‡ ä¸ªç« èŠ‚åˆ©ç”¨ python çš„åŸºç¡€åº“å®ç°ç½‘ç»œæ•°æ®çš„è·å–ã€è§£æ„ä»¥åŠå­˜å‚¨ï¼ŒåŒæ—¶ä¹Ÿå®Œæˆäº†ç®€å•çš„æ•°æ®è¯»å–æ“ä½œã€‚åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ä½¿ç”¨äº†å…¶ä»–äººå®Œæˆçš„åŠŸèƒ½åº“æ¥åŠ å¿«æˆ‘ä»¬çš„çˆ¬è™«å®ç°è¿‡ç¨‹ï¼Œå¯¹äºçˆ¬è™«ä¹Ÿæœ‰ç›¸åº”çš„ python æ¡†æ¶ä¾›æˆ‘ä»¬ä½¿ç”¨ã€Œä¸é‡å¤é€ è½®å­æ˜¯ç¨‹åºå‘˜çš„ä¸€å¤§ç‰¹ç‚¹ã€ï¼Œå½“æˆ‘ä»¬äº†è§£çˆ¬è™«çš„å®ç°è¿‡ç¨‹ä»¥åå°±å¯ä»¥å°è¯•ä½¿ç”¨æ¡†æ¶æ¥å®Œæˆè‡ªå·±çš„çˆ¬è™«ï¼ŒåŠ å¿«å¼€å‘é€Ÿåº¦ã€‚</p>

<p>åœ¨ python ä¸­æ¯”è¾ƒå¸¸ç”¨çš„çˆ¬è™«æ¡†æ¶æœ‰ Scrapy å’Œ PySpiderï¼Œä»Šå¤©é’ˆå¯¹ Scrapy çˆ¬è™«æ¡†æ¶æ¥å®ç°å‰é¢å‡ ç¯‡æ‰€å®ç°çš„åŠŸèƒ½ã€‚</p>

<h2 id="å‡†å¤‡å·¥ä½œ">å‡†å¤‡å·¥ä½œ</h2>
<p>é¦–å…ˆéœ€è¦åœ¨ç³»ç»Ÿä¸­å®‰è£… Scrapy ã€Œä¹Ÿå¯ä»¥ä½¿ç”¨ virtualenv åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿç¯å¢ƒã€ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼æ¥å®‰è£… Scrapyã€‚</p>
<pre><code class="language-python">#ä½¿ç”¨ pip æ¥å®‰è£… Scrapy
pip install Scrapy
</code></pre>
<p>Scrapy å®‰è£…å®Œæˆä»¥åï¼Œé€šè¿‡ä»¥ä¸‹æ–¹å¼æ¥åˆ›å»ºä¸€ä¸ªåŸºæœ¬çš„ Scrapy é¡¹ç›®ã€‚</p>
<pre><code class="language-python">scrapy startproject project
</code></pre>

<h2 id="ç¼–å†™ä½ çš„çˆ¬è™«">ç¼–å†™ä½ çš„çˆ¬è™«</h2>
<p>åœ¨ Scrapy ä¸­æ‰€æœ‰çš„çˆ¬è™«ç±»å¿…é¡»æ˜¯ scrapy.Spider çš„å­ç±»ï¼Œä½ å¯ä»¥è‡ªå®šä¹‰è¦å‘å‡ºçš„åˆå§‹è¯·æ±‚ï¼Œé€‰æ‹©å¦‚ä½•è·Ÿè¸ªé¡µé¢ä¸­çš„é“¾æ¥ï¼Œä»¥åŠå¦‚ä½•è§£æä¸‹è½½çš„é¡µé¢å†…å®¹ä»¥æå–æ•°æ®ã€‚</p>
<h3 id="ä¸€ä¸ªåŸºç¡€çˆ¬è™«">ä¸€ä¸ªåŸºç¡€çˆ¬è™«</h3>
<p>ç¬¬ä¸€ä¸ªçˆ¬è™«æˆ‘ä»¬é€‰æ‹©ä½¿ç”¨ scrapy.Spider ä½œä¸ºçˆ¶ç±»ï¼Œå»ºç«‹ä¸€ä¸ªç®€å•çš„å•é¡µé¢çˆ¬è™«ã€‚å»ºç«‹ä¸€ä¸ª Scrapy çˆ¬è™«æ–‡ä»¶å¯ä»¥ç›´æ¥åœ¨ spider ç›®å½•ä¸‹æ–°å»ºæ–‡ä»¶ç„¶åæ‰‹åŠ¨ç¼–å†™ç›¸å…³å†…å®¹ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨ <code>scrapy genspider [options] &lt;name&gt; &lt;domain&gt;</code> å‘½ä»¤æ¥å»ºç«‹ä¸€ä¸ªç©ºç™½æ¨¡æ¿çš„çˆ¬è™«æ–‡ä»¶ï¼Œæ–‡ä»¶å†…å®¹å¦‚ä¸‹ï¼š</p>
<pre><code class="language-python"># -*- coding: utf-8 -*-
import scrapy


class TestSpider(scrapy.Spider):
    name = 'test'
    allowed_domains = ['domain.com']
    start_urls = ['http://domain.com/']

    def parse(self, response):
        pass
</code></pre>
<p>å¦‚ä¸Šæ‰€ç¤º TestSpider ç»§æ‰¿è‡ª scrapy.Spiderï¼Œå¹¶å®šä¹‰äº†ä¸€äº›å±æ€§å’Œæ–¹æ³•ï¼š</p>
<ul>
  <li>nameï¼šå½“å‰çˆ¬è™«çš„åç§°ï¼Œç”¨æ¥æ ‡è¯†è¯¥çˆ¬è™«ã€‚</li>
  <li>allowed_domainsï¼šå½“å‰çˆ¬è™«æ‰€çˆ¬å–çš„åŸŸåã€‚</li>
  <li>start_urlsï¼šçˆ¬è™«å°†é¡ºåºçˆ¬å–å…¶ä¸­çš„ urlã€‚</li>
  <li>parseï¼šçˆ¬è™«çš„å›è°ƒå‡½æ•°ï¼Œç”¨æ¥å¤„ç†è¯·æ±‚çš„å“åº”å†…å®¹ï¼Œæ•°æ®è§£æé€šå¸¸åœ¨è¯¥å‡½æ•°å†…å®Œæˆã€‚</li>
</ul>

<p>æˆ‘ä»¬ä½¿ç”¨ scrapy.Spider æ¥å»ºç«‹ä¸€ä¸ªçˆ¬å–ã€Œç«‹åˆ›å•†åŸã€ä¸Šæ‰€æœ‰å…ƒä»¶åˆ†ç±»çš„çˆ¬è™«ï¼Œçˆ¬è™«åç§°å‘½åä¸º catalogï¼Œå°† start_urls æ›´æ¢ä¸º <code>https://www.szlcsc.com/catalog.html</code>ï¼Œä¸‹é¢è´´å‡ºè§£æå‡½æ•°çš„ä»£ç </p>
<pre><code class="language-python">    def parse(self, response):
        catalogs = response.xpath('//div[@class="catalog_a"]')
        for catalog in catalogs:
            catalog_dl = catalog.xpath('dl')
            for tag in catalog_dl:
                parent = tag.xpath('dt/a/text()').extract()
                childs = tag.xpath('dd/a/text()').extract()
                parent = self.catalog_filter_left(self.catalog_filter_right(parent[0]))
                yield CatalogItem(
                    parent = parent,
                    child = None,
                )
                for child in childs:
                    yield CatalogItem(
                        parent = parent,
                        child = self.catalog_filter_right(child),
                    )
</code></pre>
<p>é€šè¿‡ä»¥ä¸‹å‘½ä»¤æ¥å¯åŠ¨çˆ¬è™«ï¼Œè§‚å¯Ÿçˆ¬è™«çš„çˆ¬å–è¿‡ç¨‹åŠç»“æœã€‚</p>
<pre><code class="language-shell">scrapy crawl catalog
</code></pre>
<h3 id="é€’å½’çˆ¬è™«">é€’å½’çˆ¬è™«</h3>
<p>ä¸Šä¸€å°èŠ‚ä¸­å®ç°äº†ä¸€ä¸ªç®€å•çš„å•é¡µé¢çˆ¬è™«ï¼Œå®ƒä»…èƒ½è®¿é—®åœ¨ start_urls ä¸­åˆ—æ˜çš„é¡µé¢ï¼Œæ— æ³•ä»è·å–çš„é¡µé¢ä¸­æå–å‡ºé“¾æ¥å¹¶è·Ÿè¿›ã€‚scrapy é€šè¿‡ CrawlSpider æ¥å®ç°æŒ‰ç…§ä¸€å®šçš„è§„åˆ™ä»å½“å‰é¡µé¢ä¸­æå–å‡º urlï¼Œå¹¶è·Ÿè¿›çˆ¬å–ã€‚å¯ä»¥é€šè¿‡å‘½ä»¤ <code>scrapy genspider -t crawl test domain.com</code> æ¥æŒ‡å®šä½¿ç”¨ CrawlSpider å»ºç«‹çˆ¬è™«ã€‚ç”Ÿäº§æ–‡ä»¶å†…å®¹å¦‚ä¸‹ï¼š</p>
<pre><code class="language-python">mport scrapy
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule


class TestSpider(CrawlSpider):
    name = 'test'
    allowed_domains = ['domain.com']
    start_urls = ['http://domain.com/']

    rules = (
        Rule(LinkExtractor(allow=r'Items/'), callback='parse_item', follow=True),
    )

    def parse_item(self, response):
        item = {}
        #item['domain_id'] = response.xpath('//input[@id="sid"]/@value').get()
        #item['name'] = response.xpath('//div[@id="name"]').get()
        #item['description'] = response.xpath('//div[@id="description"]').get()
        return item
</code></pre>
<p>åŸºäº CrawlerSpider çš„çˆ¬è™«ä¸åŒä¹‹å¤„åœ¨äºå¤šäº†ä¸€ä¸ª rules çš„å±æ€§ï¼Œè¯¥å±æ€§å®šä¹‰äº†å¦‚ä½•ä»ç½‘é¡µä¸­æå– urlï¼Œå¹¶ä½¿ç”¨æŒ‡å®šçš„å›è°ƒå‡½æ•°æ¥å¤„ç†çˆ¬å–ç»“æœã€‚</p>

<p>ä½¿ç”¨é€’å½’çˆ¬è™«æ¥å®ç°ã€Œç«‹åˆ›å•†åŸã€ä¸­ç”Ÿäº§å•†çš„çˆ¬å–åœ¨åˆé€‚ä¸è¿‡äº†ï¼Œä»¥ä¸‹è´´å‡ºç›¸åº”çš„é“¾æ¥æå–è§„åˆ™å’Œå¤„ç†å‡½æ•°ã€‚</p>
<pre><code class="language-python">    rules = (
        Rule(LinkExtractor(allow=(r'https://list.szlcsc.com/brand/[0-9]+.html', )), callback='parse_item', follow=False),
    )

    def parse_item(self, response):
        brand_info_logo = response.xpath('//div[@class="brand-info-logo"]')
        brand_info_text = response.xpath('//div[@class="brand-info-text"]')
        name = brand_info_logo.xpath('h1[@class="brand-info-name"]/text()').extract()
        url = brand_info_logo.xpath('descendant::a[@class="blue"]/@href').extract()
        desc = brand_info_text.xpath('div[@class="introduce_txt"]//text()').extract()
        ...
        return BrandItem(
            name = name,
            url = url,
            desc = desc_text,
        )
</code></pre>

<h3 id="åŠ¨æ€æ•°æ®å¤„ç†">åŠ¨æ€æ•°æ®å¤„ç†</h3>
<p>çˆ¬è™«åœ¨å¤„ç†çš„è¿‡ç¨‹ä¸­ä¸å¯é¿å…çš„ä¼šé‡åˆ°åŠ¨æ€æ•°æ®çš„å¤„ç†ï¼Œã€Œç«‹åˆ›å•†åŸã€ä¸­å…ƒä»¶çš„åˆ—è¡¨é¡µé¢çš„ç¿»é¡µå³æ˜¯é€šè¿‡ ajax æ¥å®ç°çš„ï¼Œå¦‚æœä»…ä»…ä½¿ç”¨ä¸Šä¸€èŠ‚ä¸­çš„é€’å½’çˆ¬å–çš„æ–¹æ³•ï¼Œæœ‰å¾ˆå¤šçš„å…ƒä»¶å°†ä¼šè¢«æ¼æ‰ï¼Œåœ¨è¿™é‡Œå¯ä»¥ä½¿ç”¨ scrapy æ¨¡æ‹Ÿ post æ–¹æ³•æ¥å®ç°ç¿»é¡µçš„æ•ˆæœã€‚</p>

<p>åœ¨ scrapy ä¸­å‘ç½‘ç«™ä¸­æäº¤æ•°æ®ä½¿ç”¨ <code>scrapy.FormRequest</code> æ¥å®ç°ã€‚FormRequest ç±»æ‰©å±•äº†åŸº Request å…·æœ‰å¤„ç†HTMLè¡¨å•çš„åŠŸèƒ½ã€‚é€šè¿‡ FormReques å‘ç¿»é¡µ API ä¸Šæäº¤æ–°çš„é¡µé¢ä¿¡æ¯ï¼Œä»è€Œè·å–æ–°é¡µé¢ä¸­çš„ Json æ•°æ®ï¼Œé€šè¿‡è§£æ Json æ•°æ®æ¥è·å–æ•´ä¸ªç½‘ç«™ä¸­çš„å…ƒä»¶ä¿¡æ¯ã€‚</p>

<blockquote>
  <p>åŠ¨æ€ç¿»é¡µæ‰€éœ€è¦çš„ API åŠæäº¤æ•°æ®çš„æ ¼å¼åœ¨ <a href="https://mp.weixin.qq.com/s/xBEKaYyAGE_2rS8-b27uKw">å¤–è¡Œå­¦ Python çˆ¬è™« ç¬¬å…­ç¯‡ åŠ¨æ€ç¿»é¡µ</a> ä¸­åšè¿‡åˆ†æï¼Œå¯ä»¥åœ¨é‚£é‡Œæ‰¾åˆ°ç›¸å…³çš„ä¿¡æ¯ã€‚</p>
</blockquote>

<p>é€šè¿‡ FormRequest æ¥æŒ‡å®š urlã€æäº¤æ•°æ®ã€è¿”å›æ•°æ®çš„å›è°ƒå‡½æ•°ç­‰ï¼Œå…·ä½“å®ç°å¦‚ä¸‹ï¼š</p>
<pre><code class="language-python">            yield scrapy.FormRequest(url=product_post_url,
                        formdata=post_data,
                        callback=self.json_callback,
                        dont_filter = True)
</code></pre>
<blockquote>
  <p>ç”±äº Scrapy ä¸­è‡ªå¸¦äº† url å»é‡åŠŸèƒ½ï¼Œå› æ­¤éœ€åœ¨ FormRequest ä¸­è®¾ç½® <code>dont_filter = True</code>ï¼Œå¦åˆ™ FormRequest åªä¼šæ‰§è¡Œä¸€æ¬¡ã€‚</p>
</blockquote>

<h2 id="æ•°æ®çš„å­˜å‚¨">æ•°æ®çš„å­˜å‚¨</h2>
<p>Scrapy ä½¿ç”¨ Item æ¥å®šä¹‰é€šç”¨çš„è¾“å‡ºæ•°æ®æ ¼å¼ï¼Œæ•°æ®é€šè¿‡ Item åœ¨ Scrapy çš„å„ä¸ªæ¨¡å—ä¸­è¿›è¡Œä¼ é€’ï¼Œä»¥ä¸‹æ˜¯ä¸€ä¸ªç®€å•çš„ Item å®šä¹‰ï¼š</p>
<pre><code class="language-python">class BrandItem(scrapy.Item):
    name = scrapy.Field()
    url = scrapy.Field()
    desc = scrapy.Field()
</code></pre>
<p>æ•°æ®çš„å¤„ç†é€šå¸¸åœ¨ Pipeline ä¸­è¿›è¡Œï¼Œåœ¨çˆ¬è™«ä¸­è·å–çš„æ•°æ®å°†é€šè¿‡ Item ä¼ é€’åˆ° Pipeline çš„ process_item æ–¹æ³•ä¸­è¿›è¡Œå¤„ç†ï¼Œä»¥ä¸‹ä»£ç å®ç°äº†å°†æ•°æ®å­˜åœ¨ sqlite æ•°æ®åº“ä¸­ã€‚</p>
<pre><code class="language-python">class BrandPipeline(object):
    def __init__(self, database_uri):
        db.init_url(url=database_uri)
        self.save = SaveData()

    @classmethod
    def from_crawler(cls, crawler):
        return cls(
            database_uri=crawler.settings.get('SQLALCHEMY_DATABASE_URI'),
        )

    def process_item(self, item, spider):
        if spider.name == 'brand':
            self.save.save_brand(brand=item)
            raise DropItem('Drop Item: %s' % item)

        return item
</code></pre>
<p>å…¶ä¸­ from_crawler æ–¹æ³•ç”¨æ¥å†² setting æ–‡ä»¶ä¸­è·å–æ•°æ®åº“é“¾æ¥ã€‚</p>

<blockquote>
  <p>Item æŒ‰ç…§åœ¨ setting ä¸­å®šä¹‰çš„ä¼˜å…ˆçº§åœ¨å„ä¸ª Pipeline ä¸­è¿›è¡Œä¼ é€’ï¼Œå¦‚æœåœ¨æŸä¸ª Pipeline ä¸­å¯¹è¯¥ Item å¤„ç†å®Œæˆåç»­æ— éœ€å¤„ç†ï¼Œå¯ä»¥ä½¿ç”¨ DropItem æ¥ç»ˆæ­¢ Item å‘å…¶ä»–çš„ Pipeline ä¼ é€’ã€‚</p>
</blockquote>

<h2 id="åçˆ¬å¤„ç†">åçˆ¬å¤„ç†</h2>
<p>çˆ¬è™«ä¸å¯é¿å…çš„ä¼šé‡åˆ°ç½‘ç«™çš„åçˆ¬ç­–ç•¥ï¼Œä¸€èˆ¬çš„åçˆ¬ç­–ç•¥æ˜¯é™åˆ¶ IP çš„è®¿é—®é—´éš”ï¼Œåˆ¤æ–­å½“å‰çš„è®¿é—®ä»£ç†æ˜¯å¦æ€»æ˜¯çˆ¬è™«ç­‰ã€‚</p>

<p>é’ˆå¯¹ä»¥ä¸Šç­–ç•¥ï¼Œå¯ä»¥é€šè¿‡è®¾ç½®ä¸¤ä¸ªè¯·æ±‚ä¹‹é—´é—´éš”éšæœºçš„æ—¶é—´ï¼Œå¹¶è®¾ç½® User-Agent æ¥è§„é¿ä¸€éƒ¨åˆ†çš„åçˆ¬ç­–ç•¥ã€‚</p>

<p>è®¾ç½®è¯·æ±‚é—´éš”éšæœºæ—¶é—´çš„ä¸­é—´ä»¶å®ç°å¦‚ä¸‹ï¼š</p>
<pre><code class="language-python">class ScrapyTestRandomDelayMiddleware(object):
    def __init__(self, crawler):
        self.delay = crawler.spider.settings.get("RANDOM_DELAY")

    @classmethod
    def from_crawler(cls, crawler):
        return cls(crawler)

    def process_request(self, request, spider):
        delay = random.randint(0, self.delay)
        spider.logger.debug("### random delay: %s s ###" % delay)
        time.sleep(delay)
</code></pre>
<p>ç„¶ååœ¨ setting æ–‡ä»¶ä¸­å¯ç”¨è¯¥ä¸­é—´ä»¶ã€‚</p>
<pre><code class="language-python">RANDOM_DELAY = 3
DOWNLOADER_MIDDLEWARES = {
   'scrapy_test.middlewares.ScrapyTestDownloaderMiddleware': None,
   'scrapy_test.middlewares.ScrapyTestRandomDelayMiddleware': 999,
}
</code></pre>

<p>User-Agent å¯ä»¥ç›´æ¥åœ¨ setting æ–‡ä»¶ä¸­ä¿®æ”¹ï¼Œåœ¨æˆ‘ä»¬çš„æµè§ˆå™¨ä¸­æŸ¥çœ‹å½“å‰æµè§ˆå™¨çš„ User-Agentï¼Œå°† Scrapy çš„ User-Agent è®¾ç½®ä¸ºæµè§ˆå™¨çš„ User-Agentã€‚ä»¥ä¸‹æ˜¯ Chrome æµé‡ä¸­ User-Agent çš„æŸ¥æ‰¾æ–¹æ³•ã€‚
<img src="https://lg-8wz4hass-1252833766.cos.ap-shanghai.myqcloud.com/pic/å±å¹•å¿«ç…§2019-08-10ä¸‹åˆ10.51.59.png" alt="" /></p>

<blockquote>
  <p>å‰é¢éƒ½æ²¡æœ‰æåˆ°è¿‡ç½‘ç«™çš„åçˆ¬è™«ï¼Œè¿™æ¬¡æåˆ°çš„åŸå› æ˜¯çœŸçš„è¢«ã€Œç«‹åˆ›å•†åŸã€ç»™é™åˆ¶è®¿é—®äº†ã€‚</p>
</blockquote>

<h2 id="è¿è¡Œçˆ¬è™«">è¿è¡Œçˆ¬è™«</h2>
<p>ä»Šå¤©å°†å‰é¢æ‰€å®Œæˆçš„çˆ¬è™«åŠŸèƒ½ä½¿ç”¨ Scrapy è¿›è¡Œäº†ä¸€ä¸ªé‡æ„ï¼Œcatalog ä½¿ç”¨çš„æ˜¯å•é¡µçˆ¬è™«ç”¨æ¥è·å–åŸä»¶çš„åˆ†ç±»ä¿¡æ¯ï¼Œbrand æ˜¯ä¸€ä¸ªé€’å½’çˆ¬è™«ç”¨æ¥è·å–åŸä»¶ç”Ÿäº§å•†ä¿¡æ¯ï¼Œproduct æ˜¯ä¸€ä¸ªé€šè¿‡ post åŠ¨æ€è·å– json å¹¶è§£æçš„çˆ¬è™«ï¼Œä¸»è¦ç”¨æ¥è·å–æ‰€æœ‰å…ƒä»¶çš„ä¿¡æ¯ã€‚</p>

<p>æœ‰å¤šä¸ªçˆ¬è™«éœ€è¦è¿è¡Œï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹æ–¹æ³•é€ä¸ªè¿è¡Œçˆ¬è™«</p>
<pre><code class="language-python"># -*- coding:utf-8 -*-
import os

os.system("scrapy crawl brand")
os.system("scrapy crawl catalog")
os.system("scrapy crawl product")
</code></pre>
<p>å¦‚æœæƒ³åŒæ—¶è¿è¡Œå¤šä¸ªçˆ¬è™«ï¼Œä»¥ä¸‹æ–¹æ³•æ˜¯ä¸ªä¸é”™çš„é€‰æ‹©</p>
<pre><code class="language-python"># -*- coding:utf-8 -*-

from scrapy.utils.project import get_project_settings
from scrapy.crawler import CrawlerProcess

def main():
    setting = get_project_settings()
    process = CrawlerProcess(setting)
    process.crawl(brand)
    process.crawl(catalog)
    process.crawl(product)
    process.start()
</code></pre>
:ET